{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.0 Libraries and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import datetime\n",
    "import inflection\n",
    "import pandas              as pd\n",
    "import numpy               as np\n",
    "import seaborn             as sns\n",
    "import matplotlib.pyplot   as plt\n",
    "\n",
    "from scipy                 import stats\n",
    "from boruta                import BorutaPy\n",
    "from tabulate              import tabulate\n",
    "from dython.nominal        import associations\n",
    "from IPython.display       import Image\n",
    "from sklearn.metrics       import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error\n",
    "from sklearn.ensemble      import RandomForestRegressor\n",
    "from matplotlib.gridspec   import GridSpec\n",
    "from sklearn.linear_model  import LinearRegression, Lasso\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler, LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.1 Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para realizar as médias das previsões do modelo de ML\n",
    "def ml_error (model_name, y, yhat):\n",
    "    mae = mean_absolute_error(y, yhat)\n",
    "    mape = mean_absolute_percentage_error(y, yhat)\n",
    "    rmse = mean_squared_error(y, yhat)\n",
    "\n",
    "    return pd.DataFrame({'Model Name': model_name,\n",
    "                         'MAE': mae,\n",
    "                         'MAPE': mape,\n",
    "                         'RMSE': rmse}, index=[0])\n",
    "\n",
    "# Ajuste de tamanho para os gráficos criados\n",
    "sns.set(rc={'figure.figsize':(12, 10)})\n",
    "\n",
    "# Atalho para o uso das funções das bibliotecas de ajuste estatístico\n",
    "rs = RobustScaler()\n",
    "mms = MinMaxScaler()\n",
    "le = LabelEncoder()\n",
    "\n",
    "\n",
    "# Cramer's V -> fórmula para se medir a correlação de variáveis categóricas simetricamente\n",
    "def cramers_v(x, y):\n",
    "    confusion_matrix = pd.crosstab(x,y)\n",
    "    chi2 = stats.chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    phi2 = chi2/n\n",
    "    r,k = confusion_matrix.shape\n",
    "    phi2corr = max(0, phi2-((k-1)*(r-1))/(n-1))\n",
    "    rcorr = r-((r-1)**2)/(n-1)\n",
    "    kcorr = k-((k-1)**2)/(n-1)\n",
    "    return np.sqrt(phi2corr/min((kcorr-1),(rcorr-1)))\n",
    "\n",
    "\n",
    "# Theil's U -> fórmula para se medir a correlação de variáveis categóricas simetricamente\n",
    "def theils_u(x, y):\n",
    "    s_xy = conditional_entropy(x,y)\n",
    "    x_counter = Counter(x)\n",
    "    total_occurrences = sum(x_counter.values())\n",
    "    p_x = list(map(lambda n: n/total_occurrences, x_counter.values()))\n",
    "    s_x = ss.entropy(p_x)\n",
    "    if s_x == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return (s_x - s_xy) / s_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.2 Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_raw = pd.read_csv('csv_files/train.csv', low_memory=False)\n",
    "df_store_raw = pd.read_csv('csv_files/store.csv', low_memory=False)\n",
    "\n",
    "# Merging\n",
    "df_raw = pd.merge(df_sales_raw, df_store_raw, how='left', on='Store')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_old = ['Store', 'DayOfWeek', 'Date', 'Sales', 'Customers', 'Open', 'Promo',\n",
    "       'StateHoliday', 'SchoolHoliday', 'StoreType', 'Assortment',\n",
    "       'CompetitionDistance', 'CompetitionOpenSinceMonth',\n",
    "       'CompetitionOpenSinceYear', 'Promo2', 'Promo2SinceWeek',\n",
    "       'Promo2SinceYear', 'PromoInterval']\n",
    "\n",
    "snakecase = lambda x: inflection.underscore(x)\n",
    "\n",
    "cols_new = list(map(snakecase, cols_old))\n",
    "df1.columns = cols_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Data Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['date'] = pd.to_datetime(df1['date'])\n",
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Check NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 Fill NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#competition_distance -> filling the dataset with a high distance value\n",
    "df1['competition_distance'] = df1['competition_distance'].apply(lambda x: 200000.0 if math.isnan(x) else x)\n",
    "\n",
    "#competition_open_since_month\n",
    "df1['competition_open_since_month'] = df1.apply(lambda x: x['date'].month if math.isnan(x['competition_open_since_month']) else x['competition_open_since_month'], axis=1)\n",
    "\n",
    "#competition_open_since_year\n",
    "df1['competition_open_since_year'] = df1.apply(lambda x: x['date'].year if math.isnan(x['competition_open_since_year']) else x['competition_open_since_year'], axis=1)\n",
    "\n",
    "#promo2_since_week\n",
    "df1['promo2_since_week'] = df1.apply(lambda x: x['date'].week if math.isnan(x['promo2_since_week']) else x['promo2_since_week'], axis=1)\n",
    "\n",
    "#promo2_since_year\n",
    "df1['promo2_since_year'] = df1.apply(lambda x: x['date'].year if math.isnan(x['promo2_since_year']) else x['promo2_since_year'], axis=1)\n",
    "\n",
    "#promo_interval\n",
    "month_map = {1: 'Jan', 2: 'Fev', 3: 'Mar', 4: 'Apr', 5: 'May', 6: 'Jun', 7: 'Jul', 8: 'Aug', 9: 'Sep', 10: 'Oct', 11: 'Nov', 12: 'Dec'}\n",
    "\n",
    "df1['promo_interval'].fillna(0, inplace=True)\n",
    "\n",
    "df1['month_map'] = df1['date'].dt.month.map(month_map)\n",
    "\n",
    "df1['is_promo'] = df1[['promo_interval', 'month_map']].apply(lambda x: 0 if x['promo_interval'] == 0 else 1 if x['month_map'] in x['promo_interval'].split(',') else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6 Change Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['competition_open_since_month'] = df1['competition_open_since_month'].astype(int)\n",
    "df1['competition_open_since_year'] = df1['competition_open_since_year'].astype(int)\n",
    "df1['promo2_since_week'] = df1['promo2_since_week'].astype(int)\n",
    "df1['promo2_since_year'] = df1['promo2_since_year'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7 Descriptive Statistical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_attributes = df1.select_dtypes(include=['int64', 'float64'])\n",
    "cat_attributes = df1.select_dtypes(exclude=['int64', 'float64', 'datetime64[ns]'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7.1 Numerical Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = pd.DataFrame(num_attributes.apply(lambda x: x.skew())).T\n",
    "d2 = pd.DataFrame(num_attributes.apply(lambda x: x.kurtosis())).T\n",
    "d3 = pd.concat([d1, d2]).T.reset_index()\n",
    "d3.columns = ['attributes', 'skew', 'kurtosis']\n",
    "d3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(df1['competition_distance'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7.2 Categorical Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.set(rc={'figure.figsize':(15, 10)})\n",
    "aux1 = df1[(df1['state_holiday'] != 0) & (df1['sales'] > 0 )]\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.boxenplot(x='state_holiday' , y='sales' , data=aux1 )\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.boxenplot(x='store_type' , y='sales' , data=aux1 )\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.boxenplot(x='assortment' , y='sales' , data=aux1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('img/mind_map_hypthesis.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Hypothesis\n",
    "\n",
    "1. Lojas com maior sortimentos vendem mais?\n",
    "2. Lojas com competidores mais próximos vendem menos?\n",
    "3. Lojas com competidores há mais tempo vendem mais?\n",
    "4. Lojas com promoções ativas por mais tempo vendem mais?\n",
    "5. Lojas com promoções consecutivas vendem mais?\n",
    "6. Lojas abertas durante o feriado do Natal vendem mais?\n",
    "7. As lojas estão vendendo mais ao longo dos anos?\n",
    "8. Lojas vendem mais nos finais de semana?\n",
    "10. Lojas vendem mais depois do 10º dia do mês?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.copy()\n",
    "\n",
    "# year\n",
    "df2['year'] = df2['date'].dt.year\n",
    "\n",
    "# month\n",
    "df2['month'] = df2['date'].dt.month\n",
    "\n",
    "# day\n",
    "df2['day'] = df2['date'].dt.day\n",
    "\n",
    "# week of year\n",
    "df2['week_of_year'] = df2['date'].dt.isocalendar().week\n",
    "# year week\n",
    "df2['year_week'] = df2['date'].dt.strftime('%Y-%W')\n",
    "\n",
    "\n",
    "# competition since\n",
    "df2['competition_since'] = df2.apply(lambda x: datetime.datetime(year= x['competition_open_since_year'], month= x['competition_open_since_month'], day=1), axis=1)    \n",
    "df2['competition_time_month'] = ((df2['date'] - df2['competition_since']) / 30).apply(lambda x: x.days).astype(int)\n",
    "\n",
    "\n",
    "# promo since\n",
    "df2['promo_since'] = df2['promo2_since_year'].astype(str) + '-' + df2['promo2_since_week'].astype(str)\n",
    "df2['promo_since'] = df2['promo_since'].apply(lambda x: datetime.datetime.strptime(x + '-1', '%Y-%W-%w' ) - datetime.timedelta(days= 7))\n",
    "df2['promo_time_week'] = ((df2['date'] - df2['promo_since']) / 7).apply(lambda x: x.days).astype(int)\n",
    "\n",
    "# assortment\n",
    "df2['assortment'] = df2['assortment'].apply(lambda x: 'basic' if x == 'a' else 'extra' if x == 'b' else 'extended')\n",
    "\n",
    "# state holiday\n",
    "df2['state_holiday'] = df2['state_holiday'].apply(lambda x: 'public_holiday' if x == 'a' else 'easter_holiday' if x == 'b' else 'christmas' if x == 'c' else 'regular_day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.sample(5).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 Data Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Row Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df3[(['open'] != 0) & (df3['sales'] > 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Column Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_drop = ['customers', 'open', 'promo_interval', 'month_map']\n",
    "df3 = df3.drop(cols_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0 EDA (Exploration Data Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df3.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Univariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1 Response Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(df4['sales'], kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2 Numerical Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_attributes.hist(bins=25, figsize=(25, 10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.3 Categorical Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4['state_holiday'].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize']=(18, 12)\n",
    "\n",
    "# State holiday\n",
    "plt.subplot(3, 2, 1)\n",
    "aux = df4[df4['state_holiday'] != 'regular_day']\n",
    "sns.countplot(x='state_holiday', data=aux)\n",
    "\n",
    "plt.subplot(3, 2, 2)\n",
    "sns.kdeplot(df4[df4['state_holiday'] == 'public_holiday']['sales'], label='public_holiday', fill=True)\n",
    "sns.kdeplot(df4[df4['state_holiday'] == 'easter_holiday']['sales'], label='easter_holiday', fill=True)\n",
    "sns.kdeplot(df4[df4['state_holiday'] == 'christmas']['sales'], label='christmas', fill=True)\n",
    "\n",
    "# Store Type\n",
    "plt.subplot(3, 2, 3)\n",
    "sns.countplot(data=df4, x= 'store_type')\n",
    "\n",
    "plt.subplot(3, 2, 4)\n",
    "sns.kdeplot(df4[df4['store_type'] == 'a']['sales'], label='a', fill=True)\n",
    "sns.kdeplot(df4[df4['store_type'] == 'b']['sales'], label='b', fill=True)\n",
    "sns.kdeplot(df4[df4['store_type'] == 'c']['sales'], label='c', fill=True)\n",
    "sns.kdeplot(df4[df4['store_type'] == 'd']['sales'], label='d', fill=True)\n",
    "\n",
    "# Assortment\n",
    "plt.subplot(3, 2, 5)\n",
    "sns.countplot(data=df4, x= 'assortment')\n",
    "\n",
    "plt.subplot(3, 2, 6)\n",
    "sns.kdeplot(df4[df4['assortment'] == 'basic']['sales'], label='basic', fill=True)\n",
    "sns.kdeplot(df4[df4['assortment'] == 'extra']['sales'], label='extra', fill=True)\n",
    "sns.kdeplot(df4[df4['assortment'] == 'extended']['sales'], label='extended', fill=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Bivariate Analysis (Hypothsis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Lojas com maior sortimentos vendem mais?\n",
    "\n",
    "-> A hipótese é FALSA. As lojas que possuem maior sortimento VENDEM MENOS no geral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize']=(15, 5);\n",
    "\n",
    "aux1 = df4[['assortment', 'sales']].groupby('assortment').sum().reset_index()\n",
    "sns.barplot(x='assortment', y='sales', data=aux1);\n",
    "\n",
    "aux2 = df4[['year_week', 'assortment', 'sales']].groupby(['year_week', 'assortment']).sum().reset_index()\n",
    "aux2.pivot(index='year_week', columns='assortment', values='sales').plot();\n",
    "\n",
    "aux3 = aux2[aux2['assortment'] == 'extra']\n",
    "aux3.pivot(index='year_week', columns='assortment', values='sales').plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Lojas com competidores mais próximos vendem menos?\n",
    "-> A hipótese é FALSA. Lojas com COMPETIDORES MAIS PRÓXIMOS VENDEM MAIS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize']=(20, 7);\n",
    "\n",
    "aux1 = df4[['competition_distance', 'sales']].groupby('competition_distance').sum().reset_index()\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.scatterplot(x='competition_distance', y='sales', data=aux1);\n",
    "plt.xticks(rotation=90);\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "bins = list(np.arange(0, 20000, 1000))\n",
    "aux1['competition_distance_binned'] = pd.cut(aux1['competition_distance'], bins=bins)\n",
    "aux2 = aux1[['competition_distance_binned', 'sales']].groupby('competition_distance_binned').sum().reset_index()\n",
    "sns.barplot(x='competition_distance_binned', y='sales', data=aux2);\n",
    "plt.xticks(rotation=90);\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.heatmap(aux1.corr(method='pearson'), annot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Lojas com competidores há mais tempo vendem mais?\n",
    "->  A hipótese é FALSA. Lojas com competidores há mais tempo VENDEM MENOS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize']=(20, 10);\n",
    "\n",
    "# linha, coluna, posição\n",
    "plt.subplot(2, 1, 1)\n",
    "aux1 = df4[['competition_time_month', 'sales']].groupby('competition_time_month').sum().reset_index()\n",
    "aux2 = aux1[(aux1['competition_time_month'] < 120) & (aux1['competition_time_month'] != 0)]\n",
    "sns.barplot(x='competition_time_month', y='sales', data=aux2);\n",
    "plt.xticks(rotation=90);\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.regplot(x='competition_time_month', y='sales', data=aux2);\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.heatmap(aux1.corr(method='pearson'), annot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Lojas com promoções ativas por mais tempo vendem mais?\n",
    "-> A hipótese é FALSA. Depois de um certo período de promoção as lojas passam a vender MENOS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize']=(25, 10);\n",
    "aux1 = df4[['promo_time_week', 'sales']].groupby('promo_time_week').sum().reset_index()\n",
    "grid = GridSpec(2,3)\n",
    "\n",
    "# promo extendido\n",
    "aux2 = aux1[aux1['promo_time_week'] > 0]\n",
    "plt.subplot(grid[0,0])\n",
    "sns.barplot(x='promo_time_week', y='sales', data=aux2);\n",
    "plt.xticks(rotation=90);\n",
    "plt.subplot(grid[0,1])\n",
    "sns.regplot(x='promo_time_week', y='sales', data=aux2);\n",
    "\n",
    "# promo regular\n",
    "aux3 = aux1[aux1['promo_time_week'] < 0]\n",
    "plt.subplot(grid[1,0])\n",
    "sns.barplot(x='promo_time_week', y='sales', data=aux3);\n",
    "plt.xticks(rotation=90);\n",
    "plt.subplot(grid[1,1])\n",
    "sns.regplot(x='promo_time_week', y='sales', data=aux3);\n",
    "\n",
    "# correlação\n",
    "plt.subplot(grid[2:])\n",
    "sns.heatmap(aux1.corr(method='pearson'), annot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. Lojas com promoções consecutivas vendem mais?\n",
    "-> A hipótese é FALSA. As lojas com promoções consecutivas vendem MENOS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.rcParams['figure.figsize']=(16, 8);\n",
    "aux1 = df4[(df4['promo'] == 1) & (df4['promo2'] == 1)][['year_week', 'sales']].groupby('year_week').sum().reset_index()\n",
    "#ax = aux1.plot()\n",
    "aux2 = df4[(df4['promo'] == 1) & (df4['promo2'] == 0)][['year_week', 'sales']].groupby('year_week').sum().reset_index()\n",
    "#aux2.plot(ax=ax)\n",
    "#ax.legend(labels=['Tradicional e Extendida', 'Extendida']);\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.lineplot(x=\"year_week\", y=\"sales\", color='blue', data=aux1, ax=ax);\n",
    "sns.lineplot(x=\"year_week\", y=\"sales\", color='red', data=aux2, ax=ax);\n",
    "plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. Lojas abertas durante o feriado do Natal vendem mais?\n",
    "-> A hipótese é FALSA. As lojas abertas durante o feriado do Natal vendem MENOS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "aux1 = df4[df4['state_holiday'] != 'regular_day'][['state_holiday', 'sales']].groupby('state_holiday').sum().reset_index() \n",
    "sns.barplot(x='state_holiday', y='sales', data=aux1);\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "aux2 = df4[df4['state_holiday'] != 'regular_day'][['state_holiday', 'year', 'sales']].groupby(['state_holiday', 'year']).sum().reset_index() \n",
    "sns.barplot(x='year', y='sales', hue='state_holiday', data=aux2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7. As lojas estão vendendo mais ao longo dos anos?\n",
    "-> A hipótese é FALSA. As vendas estão diminuindo ao longo dos anos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 3, 1)\n",
    "aux1 = df4[['year', 'sales']].groupby('year').sum().reset_index()\n",
    "sns.barplot(x='year', y='sales', data=aux1);\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "aux1 = df4[['year', 'sales']].groupby('year').sum().reset_index()\n",
    "sns.regplot(x='year', y='sales', data=aux1)\n",
    "plt.xticks(rotation=90);\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "aux1 = df4[['year', 'sales']].groupby('year').sum().reset_index()\n",
    "sns.heatmap(aux1.corr(method='pearson'), annot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8. Lojas vendem menos nos finais de semana?\n",
    "-> A hipótese é VERDADEIRA. As lojas vendem MENOS nos finais de semana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSpec(1,3)\n",
    "\n",
    "plt.subplot(grid[0,0])\n",
    "aux1 = df4[['day_of_week', 'sales']].groupby('day_of_week').sum().reset_index()\n",
    "sns.barplot(x='day_of_week', y='sales', data=aux1);\n",
    "\n",
    "plt.subplot(grid[0,1])\n",
    "aux1 = df4[['day_of_week', 'sales']].groupby('day_of_week').sum().reset_index()\n",
    "sns.regplot(x='day_of_week', y='sales', data=aux1)\n",
    "plt.xticks(rotation=90);\n",
    "\n",
    "plt.subplot(grid[0,2])\n",
    "aux1 = df4[['day_of_week', 'sales']].groupby('day_of_week').sum().reset_index()\n",
    "sns.heatmap(aux1.corr(method='pearson'), annot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9. Lojas vendem mais depois do 10º dia do mês?\n",
    "-> A hipótese é VERDADEIRA. As lojas vendem MAIS depois do 10º do mês."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSpec(2,3)\n",
    "\n",
    "plt.subplot(grid[0,0])\n",
    "aux1 = df4[['day', 'sales']].groupby('day').sum().reset_index()\n",
    "sns.barplot(x='day', y='sales', data=aux1)\n",
    "\n",
    "plt.subplot(grid[0,1])\n",
    "aux1 = df4[['day', 'sales']].groupby('day').sum().reset_index()\n",
    "sns.regplot(x='day', y='sales', data=aux1)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.subplot(grid[0,2])\n",
    "aux1 = df4[['day', 'sales']].groupby('day').sum().reset_index()\n",
    "sns.heatmap(aux1.corr(method='pearson'), annot=True)\n",
    "\n",
    "aux1['before_after'] = aux1['day'].apply(lambda x: 'before_day_10' if x <= 10 else 'after_day_10')\n",
    "aux2 = aux1[['before_after', 'sales']].groupby('before_after').sum().reset_index()\n",
    "\n",
    "plt.subplot(grid[1,:])\n",
    "sns.barplot(x='before_after', y='sales', data=aux2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Resumo da checagem de Hipóteses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab =[['Hipoteses', 'Conclusao', 'Relevancia'],\n",
    "['H1', 'Falsa', 'Baixa'],\n",
    "['H2', 'Falsa', 'Media'],\n",
    "['H3', 'Falsa', 'Media'],\n",
    "['H4', 'Falsa', 'Baixa'],\n",
    "['H5', 'Falsa', 'Baixa'],\n",
    "['H6', 'Falsa', 'Media'],\n",
    "['H7', 'Falsa', 'Alta'],\n",
    "['H8', 'Verdadeira', 'Alta'],\n",
    "['H9', 'Verdadeira', 'Alta']]\n",
    "print( tabulate( tab, headers='firstrow' ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Multivariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.1 Numerical Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = num_attributes.corr(method='pearson')\n",
    "sns.heatmap(correlation, annot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.2 Categorical Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separação dos valores categóricos\n",
    "aux = df4.select_dtypes(include='object')\n",
    "\n",
    "# Aplicação da correlação categórica de Cramer's V\n",
    "aux1 = cramers_v(aux['state_holiday'], aux['state_holiday'])\n",
    "aux2 = cramers_v(aux['state_holiday'], aux['store_type'])\n",
    "aux3 = cramers_v(aux['state_holiday'], aux['assortment'])\n",
    "\n",
    "aux4 = cramers_v(aux['store_type'], aux['state_holiday'])\n",
    "aux5 = cramers_v(aux['store_type'], aux['store_type'])\n",
    "aux6 = cramers_v(aux['store_type'], aux['assortment'])\n",
    "\n",
    "aux7 = cramers_v(aux['assortment'], aux['state_holiday'])\n",
    "aux8 = cramers_v(aux['assortment'], aux['store_type'])\n",
    "aux9 = cramers_v(aux['assortment'], aux['assortment'])\n",
    "\n",
    "aux10 = pd.DataFrame({'state_holiday': [aux1, aux2, aux3],\n",
    "                      'store_type': [aux4, aux5, aux6], \n",
    "                      'assortment': [aux7, aux8, aux9]})\n",
    "\n",
    "aux10 = aux10.set_index(aux10.columns)\n",
    "\n",
    "sns.heatmap(aux10, annot=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicação da correlação categórica de Theil's U (uncertainty coefficient)\n",
    "associations(aux10, nom_nom_assoc='theil', figsize=(10,10));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.0 Data Preparation for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df4.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Não há atributos para normalizar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Rescaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = df5.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "# ajuste da coluna 'competition_distance' -> RobustScaler\n",
    "df5['competition_distance'] = rs.fit_transform(df5[['competition_distance']].values)\n",
    "\n",
    "# ajuste da coluna 'competition_time_month' -> RobustScaler\n",
    "df5['competition_time_month'] = rs.fit_transform(df5[['competition_time_month']].values)\n",
    "\n",
    "# ajuste da coluna 'promo_time_week' -> MinMaxScaler\n",
    "df5['promo_time_week'] = mms.fit_transform(df5[['promo_time_week']].values)\n",
    "\n",
    "# ajuste da coluna 'year' -> MinMaxScaler\n",
    "df5['year'] = mms.fit_transform(df5[['year']].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.3.1 Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformação dos conteúdos 'object' para 'int64' -> One Hot Encoding\n",
    "df5 = pd.get_dummies(df5, prefix=['state_holiday'], columns=['state_holiday'])\n",
    "\n",
    "# transformção dos conteúdos 'object' para 'int64' -> Label Encoding\n",
    "df5['store_type'] = le.fit_transform(df5['store_type'])\n",
    "\n",
    "# transformação dos conteúdos 'object' para 'int64' -> Ordinal Encoding\n",
    "assortment_dict = {'basic': 1, 'extra': 2, 'extended': 3}\n",
    "df5['assortment'] = df5['assortment'].map(assortment_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.3.2 Response Variable Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5['sales'] = np.log1p(df5['sales'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.3.3 Nature Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ajuste dos períodos de tempo dos dias do mês, dias da semana, semanas e meses para valores cíclicos utilizando as fórmulas de seno e coseno\n",
    "# colunas dos dias da semana\n",
    "df5['day_of_week_sin'] = df5['day_of_week'].apply(lambda x: np.sin(x * (2. * np.pi / 7)))\n",
    "df5['day_of_week_cos'] = df5['day_of_week'].apply(lambda x: np.cos(x * (2. * np.pi / 7)))\n",
    "\n",
    "# dias do mês\n",
    "df5['day_sin'] = df5['day'].apply(lambda x: np.sin(x * (2. * np.pi / 30)))\n",
    "df5['day_cos'] = df5['day'].apply(lambda x: np.cos(x * (2. * np.pi / 30)))\n",
    "\n",
    "# semanas\n",
    "df5['week_of_year_sin'] = df5['week_of_year'].apply(lambda x: np.sin(x * (2. * np.pi / 52)))\n",
    "df5['week_of_year_cos'] = df5['week_of_year'].apply(lambda x: np.cos(x * (2. * np.pi / 52)))\n",
    "\n",
    "# meses\n",
    "df5['month_sin'] = df5['month'].apply(lambda x: np.sin(x * (2. * np.pi / 12)))\n",
    "df5['month_cos'] = df5['month'].apply(lambda x: np.cos(x * (2. * np.pi / 12)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.0 Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = df5.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.1 Spliting dateframe for test and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_drop = ['week_of_year', 'day', 'month', 'day_of_week', 'promo_since', 'competition_since', 'year_week']\n",
    "df6 = df6.drop(cols_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6[['store', 'date']].groupby('store').max().reset_index()['date'][0] - datetime.timedelta(days=6*7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training dataset\n",
    "X_train = df6[df6['date'] < '2015-06-19']\n",
    "y_train = X_train['sales']\n",
    "\n",
    "#test dataset\n",
    "X_test = df6[df6['date'] >= '2015-06-19']\n",
    "y_test = X_test['sales']\n",
    "\n",
    "print('Training Min Date: {}'.format(X_train['date'].min()))\n",
    "print('Training Min Date: {}'.format(X_train['date'].max()))\n",
    "\n",
    "print('\\nTraining Min Date: {}'.format(X_test['date'].min()))\n",
    "print('Training Min Date: {}'.format(X_test['date'].max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.2 Boruta as Feature Selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and test dataset for Boruta\n",
    "x_train_n = X_train.drop(['date', 'sales'], axis=1).values\n",
    "y_train_n = y_train.values.ravel()\n",
    "\n",
    "# defining RandomForestRegressor\n",
    "rf = RandomForestRegressor(n_jobs=-1)\n",
    "\n",
    "boruta = BorutaPy(rf, n_estimators='auto', verbose=2, random_state=42).fit(x_train_n, y_train_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.2.1 Best Features from Boruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_selected = boruta.support_.tolist()\n",
    "\n",
    "# best features\n",
    "x_train_fs = X_train.drop(['date', 'sales'], axis=1)\n",
    "cols_selected_boruta = x_train_fs.iloc[:, cols_selected].columns.to_list()\n",
    "\n",
    "# not selected for boruta\n",
    "cols_not_selected_boruta = np.setdiff1d(x_train_fs.columns, cols_selected_boruta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_selected_boruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_not_selected_boruta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.3 Manual Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Foi adicionada manualmente a coluna 'month_sin' que havia sido excluída pelo Boruta\n",
    "cols_selected_boruta = ['store',\n",
    "                        'promo',\n",
    "                        'store_type',\n",
    "                        'assortment',\n",
    "                        'competition_distance',\n",
    "                        'competition_open_since_month',\n",
    "                        'competition_open_since_year',\n",
    "                        'promo2',\n",
    "                        'promo2_since_week',\n",
    "                        'promo2_since_year',\n",
    "                        'competition_time_month',\n",
    "                        'promo_time_week',\n",
    "                        'day_of_week_sin',\n",
    "                        'day_of_week_cos',\n",
    "                        'day_sin',\n",
    "                        'day_cos',\n",
    "                        'month_sin',\n",
    "                        'month_cos',\n",
    "                        'week_of_year_sin',\n",
    "                        'week_of_year_cos']\n",
    "\n",
    "# columns to add\n",
    "feat_to_add = ['date', 'sales']\n",
    "\n",
    "# final features\n",
    "cols_selected_boruta.extend(feat_to_add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.0 Machine Learning Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = X_train[cols_selected_boruta]\n",
    "x_test = X_test[cols_selected_boruta]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.1 Average Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux1 = x_test.copy()\n",
    "aux1['sales'] = y_test.copy()\n",
    "\n",
    "# prediction\n",
    "aux2 = aux1[['store', 'sales']].groupby('store').mean().reset_index().rename(columns={'sales': 'predictions'})\n",
    "aux1 = pd.merge(aux1, aux2, how='left', on='store')\n",
    "yhat_baseline = aux1['predictions']\n",
    "\n",
    "# performance\n",
    "baseline_result = ml_error('Average Model', np.expm1(y_test), np.expm1(yhat_baseline))\n",
    "baseline_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.2 Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "lr = LinearRegression().fit(x_train, y_train)\n",
    "\n",
    "# prediction\n",
    "yhat_lr = lr.predict(x_test)\n",
    "\n",
    "# performance\n",
    "lr_result = ml_error('Linear Regression', np.expm1(y_test), np.expm1(yhat_lr))\n",
    "lr_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.3 Linear Regression Regularized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "lrr = Lasso(alpha=0.01).fit(x_train, y_train)\n",
    "\n",
    "# prediction\n",
    "yhat_lrr = lrr.predict(x_test)\n",
    "\n",
    "# performance\n",
    "lrr_result = ml_error('LinearRegression - Lasso', np.expm1(y_test), np.expm1(yhat_lrr))\n",
    "lrr_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.3 Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "rf = RandomForestRegressor(n_estimators=100, n_jobs=-1, random_state=42).fit(x_train, y_train)\n",
    "\n",
    "# prediction\n",
    "yhat_rf = rf.predict(x_test)\n",
    "\n",
    "# performance\n",
    "rf_result = ml_error('Random Forest Regressor', np.expm1(y_test), np.expm1(yhat_lrr))\n",
    "rf_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "a399c2ac7b5b17bf514634d9aba7b4639a4982b0f452986d5f89cbb8104322d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
